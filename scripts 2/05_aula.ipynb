{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando funções externas a API da OpenAI\n",
    "\n",
    "Um grande salto de possibilidades de utilizações únicas da LLMs ocorrreu quando a OpenAI lançou o function calling. Essa ferramenta permite adicionarmos manualmente funções externas ao modelo que ele, dependendo da situação, poderá utilizar para obter novas informações ou atuar em diversos escopos. Vamos fazer uma breve revisão de como utilizamos funções externas na api da OpenAI, este assunto é explorado mais afundo no curso de Explorando a API da OpenAI. Na próxima aula, mostraremos como o framework langChain facilita a utilização das funções externas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando função que será adicionada ao modelo\n",
    "\n",
    "Utilizaremos uma função simples que simula uma api de tempo, que retorna a temperatura de um determinado local. Lembrando que modelos de llm são treinados com dados históricos, portanto, não possuem informações atuais. A única forma de eles entenderem o que está ocorrendo neste instante é passando informações pra eles através de prompts ou de funções externas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_temperatura_atual(local, unidade=\"celsius\"):\n",
    "    if \"são paulo\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"São Paulo\", \"temperatura\": \"32\", \"unidade\": unidade}\n",
    "            )\n",
    "    elif \"porto alegre\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": unidade}\n",
    "            )\n",
    "    else:\n",
    "        return json.dumps(\n",
    "            {\"local\": local, \"temperatura\": \"unknown\"}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": \"celsius\"}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obter_temperatura_atual('Porto Alegre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando descrição da função\n",
    "\n",
    "Através dessa descrição o modelo entenderá o que a função faz e como ela pode ser utilizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"obter_temperatura_atual\",\n",
    "            \"description\": \"Obtém a temperatura atual em uma dada cidade\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"local\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"O nome da cidade. Ex: São Paulo\",\n",
    "                    },\n",
    "                    \"unidade\": {\n",
    "                        \"type\": \"string\", \n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"local\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chamando o modelo com a nova ferramenta\n",
    "\n",
    "Para chamar o modelo com a ferramenta criada, basta passar o argumento tools com uma lista de ferramentas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9S4Dz4a8Om4R33B0IGUIUzDI3Dtcb', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yOuegMyCjHRpZXkYkWFMmrUL', function=Function(arguments='{\"local\":\"Porto Alegre\"}', name='obter_temperatura_atual'), type='function')]))], created=1716476451, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=22, prompt_tokens=87, total_tokens=109))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagens = [\n",
    "    {'role': 'user', 'content': 'Qual é temperatura em Porto Alegre agora?'}\n",
    "]\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=mensagens,\n",
    "    tools=tools,\n",
    "    tool_choice='auto'\n",
    ")\n",
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando a resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos perceber que o conteúdo da resposta veio vazio, pois para a pergunta \"Qual é a temperatura em Porto Alegre?\" ele necessitará chamar a função antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yOuegMyCjHRpZXkYkWFMmrUL', function=Function(arguments='{\"local\":\"Porto Alegre\"}', name='obter_temperatura_atual'), type='function')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagem = resposta.choices[0].message\n",
    "mensagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagem.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_yOuegMyCjHRpZXkYkWFMmrUL', function=Function(arguments='{\"local\":\"Porto Alegre\"}', name='obter_temperatura_atual'), type='function')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagem.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obter_temperatura_atual\n",
      "{\"local\":\"Porto Alegre\"}\n"
     ]
    }
   ],
   "source": [
    "tools_call = mensagem.tool_calls[0]\n",
    "print(tools_call.function.name)\n",
    "print(tools_call.function.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando resultado da função as mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": \"celsius\"}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observacao = obter_temperatura_atual(**json.loads(tools_call.function.arguments))\n",
    "observacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chamando novamente o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Qual é temperatura em Porto Alegre agora?'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yOuegMyCjHRpZXkYkWFMmrUL', function=Function(arguments='{\"local\":\"Porto Alegre\"}', name='obter_temperatura_atual'), type='function')])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagens.append(mensagem)\n",
    "mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Qual é temperatura em Porto Alegre agora?'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yOuegMyCjHRpZXkYkWFMmrUL', function=Function(arguments='{\"local\":\"Porto Alegre\"}', name='obter_temperatura_atual'), type='function')]),\n",
       " {'tool_call_id': 'call_yOuegMyCjHRpZXkYkWFMmrUL',\n",
       "  'role': 'tool',\n",
       "  'name': 'obter_temperatura_atual',\n",
       "  'content': '{\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": \"celsius\"}'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagens.append({\n",
    "    'tool_call_id': tools_call.id,\n",
    "    'role': 'tool',\n",
    "    'name': tools_call.function.name,\n",
    "    'content': observacao\n",
    "})\n",
    "mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9S4IXDcfVOj869V1pS1fzXBToOrPy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='A temperatura em Porto Alegre agora é de 25°C.', role='assistant', function_call=None, tool_calls=None))], created=1716476733, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=15, prompt_tokens=147, total_tokens=162))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=mensagens,\n",
    "    tools=tools,\n",
    "    tool_choice='auto'\n",
    ")\n",
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A temperatura em Porto Alegre agora é de 25°C.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando diferentes perguntas e o parâmetro tool_choice\n",
    "\n",
    "Através do parâmetro tool_choice é possível forçar o modelo a sempre utilizar uma tool. Vamos ver como ele se comporta para diferentes perguntas modificando o parâmetro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetro \"auto\"\n",
    "\n",
    "Assim o modelo define automaticamente se é necessária a utilização de uma função ou não"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteúdo: None\n",
      "Tools: [ChatCompletionMessageToolCall(id='call_PS1Nw8caOtDo9Q48ygEumxDa', function=Function(arguments='{\"local\":\"Porto Alegre\"}', name='obter_temperatura_atual'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "    {'role': 'user', 'content': 'Qual é temperatura em Porto Alegre agora?'}\n",
    "]\n",
    "resposta = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=mensagens,\n",
    "    tools=tools,\n",
    "    tool_choice='auto'\n",
    ")\n",
    "mensagem = resposta.choices[0].message\n",
    "print('Conteúdo:', mensagem.content)\n",
    "print('Tools:', mensagem.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteúdo: Olá! Como posso ajudar você hoje?\n",
      "Tools: None\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "    {'role': 'user', 'content': 'Olá'}\n",
    "]\n",
    "resposta = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=mensagens,\n",
    "    tools=tools,\n",
    "    tool_choice='auto'\n",
    ")\n",
    "mensagem = resposta.choices[0].message\n",
    "print('Conteúdo:', mensagem.content)\n",
    "print('Tools:', mensagem.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetro \"none\"\n",
    "\n",
    "Com o parâmetro \"none\", o modelo não vai utilizar funções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteúdo: Por favor, aguarde um momento enquanto verifico a temperatura atual em Porto Alegre.\n",
      "Tools: None\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "    {'role': 'user', 'content': 'Qual a temperatura em Porto Alegre?'}\n",
    "]\n",
    "resposta = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=mensagens,\n",
    "    tools=tools,\n",
    "    tool_choice='none'\n",
    ")\n",
    "mensagem = resposta.choices[0].message\n",
    "print('Conteúdo:', mensagem.content)\n",
    "print('Tools:', mensagem.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteúdo: Olá! Como posso ajudar você hoje?\n",
      "Tools: None\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "    {'role': 'user', 'content': 'Olá'}\n",
    "]\n",
    "resposta = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=mensagens,\n",
    "    tools=tools,\n",
    "    tool_choice='none'\n",
    ")\n",
    "mensagem = resposta.choices[0].message\n",
    "print('Conteúdo:', mensagem.content)\n",
    "print('Tools:', mensagem.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetro \"function\"\n",
    "\n",
    "Podemos fazer o modelo rodar obrigatoriamente a função, passando dentro de um dicionário a função que o modelo deve rodar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteúdo: None\n",
      "Tools: [ChatCompletionMessageToolCall(id='call_EkVfWYMGPgjfBwcMr9lIEuKk', function=Function(arguments='{\"local\":\"Porto Alegre\",\"unidade\":\"celsius\"}', name='obter_temperatura_atual'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "    {'role': 'user', 'content': 'Qual a temperatura em Porto Alegre?'}\n",
    "]\n",
    "resposta = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=mensagens,\n",
    "    tools=tools,\n",
    "    tool_choice={'type': 'function', 'function':{'name': 'obter_temperatura_atual'}}\n",
    ")\n",
    "mensagem = resposta.choices[0].message\n",
    "print('Conteúdo:', mensagem.content)\n",
    "print('Tools:', mensagem.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteúdo: None\n",
      "Tools: [ChatCompletionMessageToolCall(id='call_wkJbV6ejz8feOBmoFYIOEHss', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "    {'role': 'user', 'content': 'Olá'}\n",
    "]\n",
    "resposta = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=mensagens,\n",
    "    tools=tools,\n",
    "    tool_choice={'type': 'function', 'function':{'name': 'obter_temperatura_atual'}}\n",
    ")\n",
    "mensagem = resposta.choices[0].message\n",
    "print('Conteúdo:', mensagem.content)\n",
    "print('Tools:', mensagem.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
