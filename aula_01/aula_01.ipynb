{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "O programador python se chama Lucas e é conhecido por sua habilidade excepcional em resolver problemas complexos com seu conhecimento em programação. Ele é convocado para a estação espacial Alpha, a maior e mais avançada estação espacial da galáxia.\n",
      "\n",
      "Chegando lá, Lucas é recebido pelo comandante da estação, que explica a situação. Um dos sistemas de segurança da estação, que é controlado por um programa escrito em python, está apresentando falhas e a equipe local não consegue resolver o problema. O sistema é responsável por monitorar e controlar a entrada e saída de naves na estação, e sua falha pode ser catastrófica.\n",
      "\n",
      "Lucas começa a analisar o código e logo encontra o bug responsável pelas falhas. Ele percebe que o código foi desenvolvido de forma incorreta, com uma lógica confusa e pouco eficiente. Com sua experiência, ele reescreve o código de forma mais otimizada e eficiente, eliminando o bug e garantindo o bom funcionamento do sistema de segurança.\n",
      "\n",
      "Mas Lucas não para por aí"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "question = 'Me conte uma história sobre um programador python nas estrelas. Em um futuro distante, onde a humanidade colonizou o espaço, um programador python é chamado para resolver um problema em uma estação espacial. O que acontece?'\n",
    "\n",
    "llm.invoke(question)\n",
    "\n",
    "# Itera sobre os trechos do stream e concatena na string\n",
    "for trecho in llm.stream(question):\n",
    "    print(trecho, end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chamada simultaneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "perguntas = [\n",
    "    'defina universo em 5 palavras',\n",
    "    'defina a fisica quantica em 10 palavras',\n",
    "    'Defina a teoria das cordas em 20 palavras'\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\ntudo que existe no mundo',\n",
       " '\\n\\nFenômenos microscópicos, ondas de probabilidade, dualidade onda-partícula, incerteza, estados quânticos. ',\n",
       " '\\n\\nA teoria das cordas é uma abordagem teórica que postula que as partículas fundamentais são cordas vibrantes em um espaço-tempo com mais de três dimensões.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.batch(perguntas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá! Dados sensíveis são informações pessoais que podem gerar discriminação ou preconceito se forem utilizadas de forma indevida. Alguns exemplos de dados sensíveis são informações sobre raça, etnia, opiniões políticas, religiosas, orientação sexual, saúde, entre outros.\n",
      "\n",
      "Em um processo de uma empresa do segmento de futebol, um exemplo de tratamento de dados sensíveis poderia ser a coleta e armazenamento de informações médicas de jogadores de futebol, como histórico de lesões, condição física, uso de medicamentos, entre outros. Essas informações são consideradas sensíveis devido ao seu potencial impacto na privacidade e intimidade dos indivíduos, e exigem cuidados especiais no tratamento de acordo com a LGPD.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "mensagens = [ \n",
    "    SystemMessage('Você é um assistente juridico especializado em lgpd?'),\n",
    "    HumanMessage('olá, o que sao dados sensiveis?, me traga um exemplo de tratamento em um processo de uma empresa do segmentos de futebol]')\n",
    "]\n",
    "\n",
    "resposta = chat.invoke(mensagens)\n",
    "\n",
    "print(resposta.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta.response_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trecho in chat.stream(mensagens):\n",
    "    print(trecho.content, end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usando memoria local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "mensagens = [ \n",
    "    SystemMessage('Você é um assistente juridico especializado em lgpd?'),\n",
    "    HumanMessage('olá, o que sao dados sensiveis?, me traga um exemplo de tratamento em um processo de uma empresa do segmentos de futebol]')\n",
    "]\n",
    "\n",
    "#resposta = chat.invoke(mensagens)\n",
    "\n",
    "#print(resposta.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chat.invoke(mensagens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chat.invoke(mensagens)\n",
    "\n",
    "resposta = chat.invoke(mensagens)\n",
    "\n",
    "print(resposta.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USANDO SQLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "mensagens = [ \n",
    "    SystemMessage('Você é um assistente juridico especializado em lgpd?'),\n",
    "    HumanMessage('olá, o que sao dados sensiveis?, me traga um exemplo de tratamento em um processo de uma empresa do segmentos de futebol]')\n",
    "]\n",
    "\n",
    "#resposta = chat.invoke(mensagens)\n",
    "\n",
    "#print(resposta.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path = 'arquivosDB/llm_cache.sqlite'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chat.invoke(mensagens)\n",
    "\n",
    "resposta = chat.invoke(mensagens)\n",
    "\n",
    "print(resposta.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
